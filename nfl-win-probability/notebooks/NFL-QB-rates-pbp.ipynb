{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyathena import connect\n",
    "from pyathena.pandas_cursor import PandasCursor\n",
    "\n",
    "#from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVR, LinearSVC\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "import warnings\n",
    "\n",
    "#warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "pd.options.display.max_columns = 30\n",
    "pd.options.display.max_rows = 50\n",
    "\n",
    "# Define connection to DB\n",
    "conn = connect(\n",
    "    s3_staging_dir='s3://aws-athena-query-results-323906537337-us-east-1/',\n",
    "    region_name='us-east-1',\n",
    "    cursor_class=PandasCursor\n",
    "    )\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 pbp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 165439 entries, 0 to 165438\n",
      "Data columns (total 28 columns):\n",
      " #   Column                    Non-Null Count   Dtype  \n",
      "---  ------                    --------------   -----  \n",
      " 0   season                    165439 non-null  Int64  \n",
      " 1   week                      165439 non-null  Int64  \n",
      " 2   gamecode                  165439 non-null  Int64  \n",
      " 3   eventtypeid               165439 non-null  Int64  \n",
      " 4   gameTime                  165439 non-null  Int64  \n",
      " 5   homeTeam                  165439 non-null  Int64  \n",
      " 6   awayTeam                  165439 non-null  Int64  \n",
      " 7   playid                    165439 non-null  float64\n",
      " 8   driveid                   165439 non-null  Int64  \n",
      " 9   period                    165439 non-null  Int64  \n",
      " 10  secondsRemainingInPeriod  165439 non-null  float64\n",
      " 11  startpossessionteamid     165439 non-null  Int64  \n",
      " 12  endpossessionteamid       165439 non-null  Int64  \n",
      " 13  defenseTeam               165439 non-null  Int64  \n",
      " 14  down                      165439 non-null  Int64  \n",
      " 15  yardsToGo                 165439 non-null  float64\n",
      " 16  startYardsFromGoal        165439 non-null  float64\n",
      " 17  endYardsFromGoal          165439 non-null  float64\n",
      " 18  awayscorebefore           165439 non-null  Int64  \n",
      " 19  awayscoreafter            165439 non-null  Int64  \n",
      " 20  homescorebefore           165439 non-null  Int64  \n",
      " 21  homescoreafter            165439 non-null  Int64  \n",
      " 22  iscontinuation            165439 non-null  bool   \n",
      " 23  playtypeid                165439 non-null  Int64  \n",
      " 24  playName                  165439 non-null  object \n",
      " 25  playerid                  165121 non-null  Int64  \n",
      " 26  rolloutlocation           163120 non-null  object \n",
      " 27  handofftype               67937 non-null   object \n",
      "dtypes: Int64(19), bool(1), float64(5), object(3)\n",
      "memory usage: 37.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# a work around -- issue with QB parsing for each play, since a snap player could be other than a QB\n",
    "#\n",
    "# playtypeid:\n",
    "# 1 = pass attempt\n",
    "# 2 = incomplete pass attempt\n",
    "# 3 = sack\n",
    "# 4 = designed rush OR scramble\n",
    "# 9 = intercepted pass attempt\n",
    "\n",
    "simple_query = f'''\n",
    "select\n",
    "    cast(season as integer) season,\n",
    "    eventmetadata.week, eventmetadata.gamecode, eventmetadata.eventtypeid, eventmetadata.gamedateutcepoch gameTime,\n",
    "    if(teammetadata[1].ishometeam, teammetadata[1].teamid, teammetadata[2].teamid) homeTeam,\n",
    "    if(teammetadata[2].ishometeam, teammetadata[1].teamid, teammetadata[2].teamid) awayTeam,\n",
    "    \n",
    "    playid, pbp.driveid, pbp.period,  \n",
    "    cast(secondsremaininginperiod as double) secondsRemainingInPeriod, \n",
    "    \n",
    "    pbp.startpossessionteamid, pbp.endpossessionteamid,\n",
    "    if(pbp.startpossessionteamid = teammetadata[1].teamid, teammetadata[2].teamid, teammetadata[1].teamid) defenseTeam,\n",
    "    \n",
    "    pbp.down, \n",
    "    cast(pbp.distance as double) yardsToGo,\n",
    "    cast(pbp.startyardsfromgoal as double) startYardsFromGoal, \n",
    "    cast(pbp.endyardsfromgoal as double) endYardsFromGoal,\n",
    "    pbp.awayscorebefore, pbp.awayscoreafter, pbp.homescorebefore, pbp.homescoreafter, \n",
    "    pbp.iscontinuation, \n",
    "    -- pbp.penaltytype.penaltytypeid, pbp.penaltytype.name, \n",
    "    pbp.playtype.playtypeid, pbp.playtype.name playName,\n",
    "    \n",
    "    -- parse info for QB\n",
    "    xinfo.playersnappedto playerid,\n",
    "    \n",
    "    xinfo.rolloutlocation.abbreviation rolloutlocation,\n",
    "    xinfo.handofftype.abbreviation handofftype\n",
    "from \n",
    "    datalakefootball.pbp_xinfo_enriched\n",
    "where \n",
    "    season>='2015' and \n",
    "    eventmetadata.eventtypeid in (1,2) and \n",
    "    pbp.period <= 4 and\n",
    "    pbp.down is not null and \n",
    "    pbp.playtype.playtypeid in (1,2,3,4,9)\n",
    "order by season, eventmetadata.week, eventmetadata.gamecode, pbp.period, playid, secondsremaininginperiod desc\n",
    "'''\n",
    "\n",
    "if True:\n",
    "    pbp_df = cursor.execute(simple_query).as_pandas()\n",
    "    print(pbp_df.info())\n",
    "else:\n",
    "    print(\"Failed to query!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 165121 entries, 0 to 165299\n",
      "Data columns (total 29 columns):\n",
      " #   Column                    Non-Null Count   Dtype  \n",
      "---  ------                    --------------   -----  \n",
      " 0   season                    165121 non-null  Int64  \n",
      " 1   week                      165121 non-null  Int64  \n",
      " 2   gamecode                  165121 non-null  Int64  \n",
      " 3   eventtypeid               165121 non-null  Int64  \n",
      " 4   gameTime                  165121 non-null  Int64  \n",
      " 5   homeTeam                  165121 non-null  Int64  \n",
      " 6   awayTeam                  165121 non-null  Int64  \n",
      " 7   playid                    165121 non-null  float64\n",
      " 8   driveid                   165121 non-null  Int64  \n",
      " 9   period                    165121 non-null  Int64  \n",
      " 10  secondsRemainingInPeriod  165121 non-null  float64\n",
      " 11  startpossessionteamid     165121 non-null  Int64  \n",
      " 12  endpossessionteamid       165121 non-null  Int64  \n",
      " 13  defenseTeam               165121 non-null  Int64  \n",
      " 14  down                      165121 non-null  Int64  \n",
      " 15  yardsToGo                 165121 non-null  float64\n",
      " 16  startYardsFromGoal        165121 non-null  float64\n",
      " 17  endYardsFromGoal          165121 non-null  float64\n",
      " 18  awayscorebefore           165121 non-null  Int64  \n",
      " 19  awayscoreafter            165121 non-null  Int64  \n",
      " 20  homescorebefore           165121 non-null  Int64  \n",
      " 21  homescoreafter            165121 non-null  Int64  \n",
      " 22  iscontinuation            165121 non-null  bool   \n",
      " 23  playtypeid                165121 non-null  Int64  \n",
      " 24  playName                  165121 non-null  object \n",
      " 25  playerid                  165121 non-null  Int64  \n",
      " 26  rolloutlocation           163119 non-null  object \n",
      " 27  handofftype               67935 non-null   object \n",
      " 28  scoreDiff                 165121 non-null  float64\n",
      "dtypes: Int64(19), bool(1), float64(6), object(3)\n",
      "memory usage: 39.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# remove rows without player id\n",
    "pbp_df = pbp_df[~pbp_df.playerid.isnull()]\n",
    "\n",
    "# create score difference: offenseTeam - defenseTeam\n",
    "pbp_df['scoreDiff'] = pbp_df.homescorebefore - pbp_df.awayscorebefore\n",
    "\n",
    "id = (pbp_df.startpossessionteamid == pbp_df.awayTeam).tolist()\n",
    "\n",
    "pbp_df.loc[id, 'scoreDiff'] = -pbp_df.loc[id, 'scoreDiff']\n",
    "\n",
    "pbp_df.scoreDiff = pbp_df.scoreDiff.astype('float64')\n",
    "\n",
    "#pbp_df.down = pbp_df.down.astype('float64')\n",
    "\n",
    "pbp_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### testing\n",
    "print('pass         ', sum(pbp_df.playtypeid.isin([1,2,9])) / len(pbp_df.playtypeid))\n",
    "print('rush/scramble', sum(pbp_df.playtypeid.isin([4])) / len(pbp_df.playtypeid))\n",
    "print('sack         ', sum(pbp_df.playtypeid.isin([3])) / len(pbp_df.playtypeid) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Feature / label preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_pass = pbp_df.playtypeid.isin([1,2,9])\n",
    "\n",
    "label_sack = pbp_df.playtypeid.isin([3])\n",
    "\n",
    "label_rush = pbp_df.playtypeid.isin([4]) & (pbp_df.rolloutlocation=='N')\n",
    "\n",
    "label_scramble = pbp_df.playtypeid.isin([4]) & (pbp_df.handofftype=='Q') & (pbp_df.rolloutlocation!='N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('bool')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_pass.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 165121 entries, 0 to 165120\n",
      "Data columns (total 12 columns):\n",
      " #   Column                    Non-Null Count   Dtype  \n",
      "---  ------                    --------------   -----  \n",
      " 0   scoreDiff                 165121 non-null  float64\n",
      " 1   yardsToGo                 165121 non-null  float64\n",
      " 2   startYardsFromGoal        165121 non-null  float64\n",
      " 3   secondsRemainingInPeriod  165121 non-null  float64\n",
      " 4   period_1                  165121 non-null  float64\n",
      " 5   period_2                  165121 non-null  float64\n",
      " 6   period_3                  165121 non-null  float64\n",
      " 7   period_4                  165121 non-null  float64\n",
      " 8   down_1                    165121 non-null  float64\n",
      " 9   down_2                    165121 non-null  float64\n",
      " 10  down_3                    165121 non-null  float64\n",
      " 11  down_4                    165121 non-null  float64\n",
      "dtypes: float64(12)\n",
      "memory usage: 15.1 MB\n"
     ]
    }
   ],
   "source": [
    "num_fields = [\n",
    "                'scoreDiff',\n",
    "                'yardsToGo',\n",
    "                'startYardsFromGoal',\n",
    "                'secondsRemainingInPeriod',\n",
    "             ]\n",
    "\n",
    "cat_fields = [\n",
    "                #'eventtypeid',\n",
    "                'period',\n",
    "                'down',\n",
    "             ]\n",
    "              \n",
    "# StandardScaler version\n",
    "# StandardScaler should not be applied to testing data\n",
    "transform_pipeline = ColumnTransformer(transformers=[\n",
    "                                            ('num', StandardScaler(), num_fields),\n",
    "                                            ('cat', OneHotEncoder(categories='auto'), cat_fields)\n",
    "                                        ])\n",
    "features_transformed = transform_pipeline.fit_transform(pbp_df)\n",
    "\n",
    "feature_names = num_fields.copy()\n",
    "cat_one_hot_fields = list(transform_pipeline.named_transformers_.cat.get_feature_names(input_features=cat_fields))\n",
    "feature_names.extend(cat_one_hot_fields)\n",
    "\n",
    "if type(features_transformed) == np.ndarray:\n",
    "    features_transformed = pd.DataFrame(features_transformed, columns=feature_names)\n",
    "else:\n",
    "    features_transformed = pd.DataFrame(features_transformed.toarray(), columns=feature_names)\n",
    "\n",
    "\n",
    "# None-StandardScaler version\n",
    "transform_pipeline_2 = ColumnTransformer(transformers=[\n",
    "                                            ('num', 'passthrough', num_fields),\n",
    "                                            ('cat', OneHotEncoder(categories='auto'), cat_fields)\n",
    "                                        ])\n",
    "features_transformed_2 = transform_pipeline_2.fit_transform(pbp_df)\n",
    "\n",
    "if type(features_transformed_2) == np.ndarray:\n",
    "    features_transformed_2 = pd.DataFrame(features_transformed_2, columns=feature_names)\n",
    "else:\n",
    "    features_transformed_2 = pd.DataFrame(features_transformed_2.toarray(), columns=feature_names)\n",
    "\n",
    "features_transformed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 165121 entries, 0 to 165120\n",
      "Data columns (total 12 columns):\n",
      " #   Column                    Non-Null Count   Dtype  \n",
      "---  ------                    --------------   -----  \n",
      " 0   scoreDiff                 165121 non-null  float64\n",
      " 1   yardsToGo                 165121 non-null  float64\n",
      " 2   startYardsFromGoal        165121 non-null  float64\n",
      " 3   secondsRemainingInPeriod  165121 non-null  float64\n",
      " 4   period_1                  165121 non-null  float64\n",
      " 5   period_2                  165121 non-null  float64\n",
      " 6   period_3                  165121 non-null  float64\n",
      " 7   period_4                  165121 non-null  float64\n",
      " 8   down_1                    165121 non-null  float64\n",
      " 9   down_2                    165121 non-null  float64\n",
      " 10  down_3                    165121 non-null  float64\n",
      " 11  down_4                    165121 non-null  float64\n",
      "dtypes: float64(12)\n",
      "memory usage: 15.1 MB\n"
     ]
    }
   ],
   "source": [
    "features_transformed_2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate models\n",
    "folds = 5\n",
    "\n",
    "model_logistic = LogisticRegression(solver='lbfgs', fit_intercept=True, max_iter=10000, tol=1e-4, random_state=42)\n",
    "\n",
    "model_SGD = SGDClassifier(loss='log', penalty=None, fit_intercept=True, max_iter=10000, tol=1e-4, random_state=42)\n",
    "\n",
    "model_SVC = LinearSVC(max_iter=10000, tol=1e-4, random_state=42)\n",
    "\n",
    "model_RF = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42)\n",
    "\n",
    "id_train = (pbp_df.season <= 2018).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression model\n",
    "\n",
    "scores_logistic = cross_validate(model_logistic,\n",
    "    features_transformed[id_train],\n",
    "    label_pass[id_train],\n",
    "    cv=folds,\n",
    "    scoring=(['recall','precision','f1','roc_auc']))\n",
    "print('Pass - Linear logistic regression (P/R/F1/ROC): {:.1%}  {:.1%}  {:.1%} {:.1%}'.format(scores_logistic['test_recall'].mean(), \n",
    "                                                                                   scores_logistic['test_precision'].mean(), \n",
    "                                                                                   scores_logistic['test_f1'].mean(),\n",
    "                                                                                  scores_logistic['test_roc_auc'].mean()))\n",
    "\n",
    "scores_logistic = cross_validate(model_logistic,\n",
    "    features_transformed[id_train],\n",
    "    label_rush[id_train],\n",
    "    cv=folds,\n",
    "    scoring=(['recall','precision','f1','roc_auc']))\n",
    "print('Rush - Linear logistic regression (P/R/F1/ROC): {:.1%}  {:.1%}  {:.1%} {:.1%}'.format(scores_logistic['test_recall'].mean(), \n",
    "                                                                                   scores_logistic['test_precision'].mean(), \n",
    "                                                                                   scores_logistic['test_f1'].mean(),\n",
    "                                                                                  scores_logistic['test_roc_auc'].mean()))\n",
    "\n",
    "# for imbalanced event, only roc_auc para makes sense\n",
    "scores_logistic = cross_validate(model_logistic,\n",
    "    features_transformed[id_train],\n",
    "    label_sack[id_train],\n",
    "    cv=folds,\n",
    "    scoring=(['roc_auc']))\n",
    "print('Sack - Linear logistic regression (P/R/F1/ROC): {:.1%}'.format(scores_logistic['test_roc_auc'].mean()))\n",
    "\n",
    "scores_logistic = cross_validate(model_logistic,\n",
    "    features_transformed[id_train],\n",
    "    label_scramble[id_train],\n",
    "    cv=folds,\n",
    "    scoring=(['roc_auc']))\n",
    "print('Scramble - Linear logistic regression (P/R/F1/ROC): {:.1%}'.format(scores_logistic['test_roc_auc'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM model\n",
    "\n",
    "scores_SVC = cross_validate(model_SVC,\n",
    "    features_transformed[id_train],\n",
    "    label_pass[id_train],\n",
    "    cv=folds,\n",
    "    scoring=(['recall','precision','f1','roc_auc']))\n",
    "print('Pass - SVC Classifier (P/R/F1/ROC): {:.1%}  {:.1%}  {:.1%} {:.1%}'.format(scores_SVC['test_recall'].mean(), \n",
    "                                                                        scores_SVC['test_precision'].mean(), \n",
    "                                                                        scores_SVC['test_f1'].mean(),\n",
    "                                                                        scores_SVC['test_roc_auc'].mean()))\n",
    "\n",
    "scores_SVC = cross_validate(model_SVC,\n",
    "    features_transformed[id_train],\n",
    "    label_rush[id_train],\n",
    "    cv=folds,\n",
    "    scoring=(['recall','precision','f1','roc_auc']))\n",
    "print('Rush - SVC Classifier (P/R/F1/ROC): {:.1%}  {:.1%}  {:.1%} {:.1%}'.format(scores_SVC['test_recall'].mean(), \n",
    "                                                                        scores_SVC['test_precision'].mean(), \n",
    "                                                                        scores_SVC['test_f1'].mean(),\n",
    "                                                                        scores_SVC['test_roc_auc'].mean()))\n",
    "\n",
    "# for imbalanced event, only roc_auc para makes sense\n",
    "scores_SVC = cross_validate(model_SVC,\n",
    "    features_transformed[id_train],\n",
    "    label_sack[id_train],\n",
    "    cv=folds,\n",
    "    scoring=(['roc_auc']))\n",
    "print('Sack - SVC Classifier (P/R/F1/ROC): {:.1%}'.format(scores_SVC['test_roc_auc'].mean()))\n",
    "\n",
    "scores_SVC = cross_validate(model_SVC,\n",
    "    features_transformed[id_train],\n",
    "    label_scramble[id_train],\n",
    "    cv=folds,\n",
    "    scoring=(['roc_auc']))\n",
    "print('Scramble - SVC Classifier (P/R/F1/ROC): {:.1%}'.format(scores_SVC['test_roc_auc'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass - Random Forest Classifier (P/R/F1/ROC): 71.5%  71.2%  71.3% 74.3%\n"
     ]
    }
   ],
   "source": [
    "scores_RF = cross_validate(model_RF,\n",
    "    features_transformed_2[id_train],\n",
    "    label_pass[id_train],\n",
    "    cv=folds,\n",
    "    scoring=(['recall','precision','f1','roc_auc']))\n",
    "print('Pass - Random Forest Classifier (P/R/F1/ROC): {:.1%}  {:.1%}  {:.1%} {:.1%}'.format(scores_RF['test_recall'].mean(), \n",
    "                                                                                scores_RF['test_precision'].mean(), \n",
    "                                                                                scores_RF['test_f1'].mean(),\n",
    "                                                                                scores_RF['test_roc_auc'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         False\n",
       "1          True\n",
       "2         False\n",
       "3          True\n",
       "4         False\n",
       "          ...  \n",
       "165295    False\n",
       "165296     True\n",
       "165297     True\n",
       "165298     True\n",
       "165299     True\n",
       "Name: playtypeid, Length: 165121, dtype: bool"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest\n",
    "\n",
    "scores_RF = cross_validate(model_RF,\n",
    "    features_transformed_2[id_train],\n",
    "    label_pass[id_train],\n",
    "    cv=folds,\n",
    "    scoring=(['recall','precision','f1','roc_auc']))\n",
    "print('Pass - Random Forest Classifier (P/R/F1/ROC): {:.1%}  {:.1%}  {:.1%} {:.1%}'.format(scores_RF['test_recall'].mean(), \n",
    "                                                                                scores_RF['test_precision'].mean(), \n",
    "                                                                                scores_RF['test_f1'].mean(),\n",
    "                                                                                scores_RF['test_roc_auc'].mean()))\n",
    "\n",
    "scores_RF = cross_validate(model_RF,\n",
    "    features_transformed_2[id_train],\n",
    "    label_rush[id_train],\n",
    "    cv=folds,\n",
    "    scoring=(['recall','precision','f1','roc_auc']))\n",
    "print('Rush - Random Forest Classifier (P/R/F1/ROC): {:.1%}  {:.1%}  {:.1%} {:.1%}'.format(scores_RF['test_recall'].mean(), \n",
    "                                                                                scores_RF['test_precision'].mean(), \n",
    "                                                                                scores_RF['test_f1'].mean(),\n",
    "                                                                                scores_RF['test_roc_auc'].mean()))\n",
    "\n",
    "# for imbalanced event, only roc_auc para makes sense\n",
    "scores_RF = cross_validate(model_RF,\n",
    "    features_transformed_2[id_train],\n",
    "    label_sack[id_train],\n",
    "    cv=folds,\n",
    "    scoring=(['roc_auc']))\n",
    "print('Sack - Random Forest Classifier (P/R/F1/ROC): {:.1%}'.format(scores_RF['test_roc_auc'].mean()))\n",
    "\n",
    "scores_RF = cross_validate(model_RF,\n",
    "    features_transformed[id_train],\n",
    "    label_scramble[id_train],\n",
    "    cv=folds,\n",
    "    scoring=(['roc_auc']))\n",
    "print('Scramble - Random Forest Classifier (P/R/F1/ROC): {:.1%}'.format(scores_RF['test_roc_auc'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check prediction average against observation average\n",
    "\n",
    "id = (pbp_df.season <= 2018).tolist()\n",
    "id_test = (pbp_df.season >= 2019).tolist()\n",
    "\n",
    "# pass\n",
    "model_logistic.fit(features_transformed[id], label_pass[id])\n",
    "re = model_logistic.predict(features_transformed[id])\n",
    "re_prob_pass = model_logistic.predict_proba(features_transformed[id])\n",
    "\n",
    "# create output file\n",
    "outmat = pbp_df.loc[id,['season','week','gamecode','eventtypeid','period','playid','secondsRemainingInPeriod','startpossessionteamid','down','yardsToGo','startYardsFromGoal','scoreDiff']]\n",
    "outmat['isPass'] = label_pass[id]\n",
    "outmat['pred_pass_prob'] = re_prob_pass[:,1]\n",
    "outmat.to_csv('QB_pass_training_data.csv', index=False)\n",
    "\n",
    "print(\"QB fitted average pass ratio: {:.2%}\".format(sum(re)/len(re)) )\n",
    "print(\"QB fitted average pass_prob ratio: {:.2%}\".format(np.mean(re_prob_pass[:,1])) )\n",
    "print(\"QB actual average pass ratio: {:.2%}\\n\".format(sum(label_pass[id])/sum(id)) )\n",
    "\n",
    "# rush\n",
    "model_logistic.fit(features_transformed[id], label_rush[id])\n",
    "re = model_logistic.predict(features_transformed[id])\n",
    "re_prob_rush = model_logistic.predict_proba(features_transformed[id])\n",
    "\n",
    "# create output file\n",
    "outmat = pbp_df.loc[id,['season','week','gamecode','eventtypeid','period','playid','secondsRemainingInPeriod','startpossessionteamid','down','yardsToGo','startYardsFromGoal','scoreDiff']]\n",
    "outmat['isRush'] = label_rush[id]\n",
    "outmat['pred_rush_prob'] = re_prob_rush[:,1]\n",
    "outmat.to_csv('QB_rush_training_data.csv', index=False)\n",
    "\n",
    "print(\"QB fitted average rush ratio: {:.2%}\".format(sum(re)/len(re)) )\n",
    "print(\"QB fitted average rush_prob ratio: {:.2%}\".format(np.mean(re_prob_rush[:,1])) )\n",
    "print(\"QB actual average rush ratio: {:.2%}\\n\".format(sum(label_rush[id])/sum(id)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scramble\n",
    "model_logistic.fit(features_transformed[id], label_scramble[id])\n",
    "re = model_logistic.predict(features_transformed[id])\n",
    "re_prob_scramble = model_logistic.predict_proba(features_transformed[id])\n",
    "\n",
    "print(\"QB fitted average scramble ratio: {:.2%}\".format(sum(re)/len(re)) )\n",
    "print(\"QB fitted average scramble_prob ratio: {:.2%}\".format(np.mean(re_prob_scramble[:,1])) )\n",
    "print(\"QB actual average scramble ratio: {:.2%}\\n\".format(sum(label_scramble[id])/sum(id)) )\n",
    "\n",
    "# create output file\n",
    "# outmat = pbp_df.loc[id,['season','week','gamecode','eventtypeid','period','playid','secondsRemainingInPeriod','startpossessionteamid','down','yardsToGo','startYardsFromGoal','scoreDiff']]\n",
    "# outmat['isScramble'] = label_scramble[id]\n",
    "# outmat['pred_scramble_prob'] = re_prob_scramble[:,1]\n",
    "# outmat.to_csv('QB_scramble_training_data.csv', index=False)\n",
    "\n",
    "\n",
    "# sack\n",
    "model_logistic.fit(features_transformed[id], label_sack[id])\n",
    "re = model_logistic.predict(features_transformed[id])\n",
    "re_prob_sack = model_logistic.predict_proba(features_transformed[id])\n",
    "\n",
    "print(\"QB fitted average sack ratio: {:.2%}\".format(sum(re)/len(re)) )\n",
    "print(\"QB fitted average sack_prob ratio: {:.2%}\".format(np.mean(re_prob_sack[:,1])) )\n",
    "print(\"QB actual average sack ratio: {:.2%}\".format(sum(label_sack[id])/sum(id)) )\n",
    "\n",
    "# create output file\n",
    "# outmat = pbp_df.loc[id,['season','week','gamecode','eventtypeid','period','playid','secondsRemainingInPeriod','startpossessionteamid','down','yardsToGo','startYardsFromGoal','scoreDiff']]\n",
    "# outmat['isSack'] = label_sack[id]\n",
    "# outmat['pred_sack_prob'] = re_prob_sack[:,1]\n",
    "# outmat.to_csv('QB_sack_training_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regression model interpretation\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "mod = sm.Logit(label_pass.values+0, features_transformed, missing='drop')\n",
    "res = mod.fit(method='newton', maxiter=1000)\n",
    "\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importance study\n",
    "\n",
    "regr = RandomForestRegressor(n_estimators=200, max_depth=40, random_state=0)\n",
    "regr.fit(features_transformed, label_pass)\n",
    "\n",
    "#cat_one_hot_fields = list(transform_pipeline.named_transformers_.cat.get_feature_names())\n",
    "#feature_score = pd.DataFrame([num_fields + cat_one_hot_fields, regr.feature_importances_], \n",
    "feature_score = pd.DataFrame([feature_names, regr.feature_importances_], \n",
    "                             index=['feature','importance']).transpose()\n",
    "feature_score.sort_values(by='importance',ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model performance simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate player difference: QB specific\n",
    "\n",
    "# group by QBid: apply the function of fit to each QB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a work around -- issue with QB parsing for each play, since a snap player could be not a QB\n",
    "\n",
    "simple_query = f'''\n",
    "select playerid, firstname, lastname, positions[1].positionid positionid, positions[1].name name\n",
    "from datalakefootball.players\n",
    "where positions[1].positionid = 8\n",
    "'''\n",
    "\n",
    "if True:\n",
    "    qb_info = cursor.execute(simple_query).as_pandas()\n",
    "    print(qb_info.info())\n",
    "else:\n",
    "    print(\"Failed to query!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the weight decay factor\n",
    "#  alpha = 0.9995\n",
    "#  alpha_team = 0.99\n",
    "\n",
    "#import time\n",
    "\n",
    "def playerRatesTesting(trainSeason_s, trainSeason_e, testSeason, features, label, pbp_df, playerids, \n",
    "                       model, alpha, alpha_team, playsThreshold, predictProb):\n",
    "    playerTrain = []\n",
    "    playerTest = []\n",
    "\n",
    "    weeks = pbp_df[pbp_df.season==testSeason].week.unique()\n",
    "\n",
    "    ### the back-testing is carried out one week at a time\n",
    "    for week in weeks:\n",
    "        \n",
    "        id = (pbp_df.season==testSeason) & (pbp_df.week==week)\n",
    "        id = id.tolist()\n",
    "        test_label = label[id].reset_index(drop=True)\n",
    "        test_features = features[id].reset_index(drop=True)\n",
    "        test_players = pbp_df.playerid[id]\n",
    "\n",
    "        test_player_df = pbp_df.loc[id, ['week','playerid','defenseTeam']]\n",
    "        test_player_df.drop_duplicates(inplace=True)\n",
    "\n",
    "        id = ((pbp_df.season==trainSeason_s) & (pbp_df.week >= week)) | ((pbp_df.season>trainSeason_s) & (pbp_df.season<=trainSeason_e)) | ((pbp_df.season==testSeason) & (pbp_df.week < week))\n",
    "        id = id.tolist()\n",
    "        train_label = label[id].reset_index(drop=True)\n",
    "        train_features = features[id].reset_index(drop=True)\n",
    "        train_players = pbp_df.playerid[id]\n",
    "        train_weights = np.array([alpha] * np.sum(id)) # be careful data type is very important!\n",
    "        \n",
    "        ### fit the model\n",
    "        model = model.fit(train_features, train_label)\n",
    "        \n",
    "        if predictProb:\n",
    "            pred = model.predict_proba(train_features)[:,1]\n",
    "        else:\n",
    "            pred = model.predict(train_features)\n",
    "\n",
    "        ### team_level results    \n",
    "        team_df = pd.DataFrame( { 'season':pbp_df.season[id], 'week':pbp_df.week[id], 'defenseTeam':pbp_df.defenseTeam[id], 'team_pred':pred, 'team_obs':label[id] } )\n",
    "        gd = team_df.groupby(by=['season','week','defenseTeam'], as_index=False)\n",
    "        team_df = gd.mean()\n",
    "\n",
    "        team_df['weights'] = [alpha_team] * team_df.shape[0]\n",
    "\n",
    "        team_means = []\n",
    "        for team in np.unique(team_df.defenseTeam):\n",
    "            id = (team_df.defenseTeam == team).tolist()\n",
    "            weights = np.cumprod([alpha_team] * np.sum(id))[::-1]\n",
    "            team_pred = np.sum(team_df.team_pred[id] * weights) / np.sum(weights)\n",
    "            team_obs  = np.sum(team_df.team_obs[id]  * weights) / np.sum(weights)\n",
    "            team_means.append([team, team_pred, team_obs])\n",
    "            \n",
    "        team_df = pd.DataFrame(team_means, columns=['defenseTeam','team_pred','team_obs'])\n",
    "\n",
    "        ### player_level results\n",
    "        players = train_players[~train_players.isna()].unique()\n",
    "        \n",
    "        if len(playerids) > 0:\n",
    "            players = [player for player in players if player in playerids]\n",
    "\n",
    "        #### determine weights for each play and calculate the weighted mean over all players\n",
    "        for player in players:\n",
    "            id = np.array(train_players) == player\n",
    "            playerWeights = np.cumprod([alpha] * np.sum(id))[::-1]\n",
    "            train_weights[id] = playerWeights\n",
    "        \n",
    "\n",
    "        ### mean over all players\n",
    "        all_mean = np.sum(pred * train_weights) / np.sum(train_weights)\n",
    "\n",
    "        print(week, all_mean, pred.mean(), len(train_label), len(train_players), train_features.shape, len(test_label), len(test_players), test_features.shape)\n",
    "        \n",
    "        ### player_level adjustment\n",
    "        for player in players:\n",
    "            id = np.array(train_players) == player\n",
    "            playerPred = pred[id]\n",
    "            obs  = train_label[id]\n",
    "            playerWeights = train_weights[id]\n",
    "\n",
    "            playerPred_mean = np.sum(playerPred * playerWeights) / np.sum(playerWeights)\n",
    "            obs_mean = np.sum(obs * playerWeights) / np.sum(playerWeights)\n",
    "            obs_simple_mean = obs.mean()\n",
    "\n",
    "            diff = obs_mean - playerPred_mean\n",
    "\n",
    "            onePlayerEntry = pd.DataFrame([[week, player, np.sum(id), all_mean, playerPred_mean, obs_mean, obs_simple_mean, diff, all_mean + diff]],\n",
    "                                     columns = ['week', 'playerid', 'trainPlayNum', 'trainAllPredictedMean', 'trainPlayerPredictedMean', 'trainPlayerObservedMean', \n",
    "                                                'trainPlayerSimpleMean', 'diff', 'predict'])\n",
    "\n",
    "            # for rookie player, adjust the prediction if the play # is too small. 50 is too large\n",
    "            if onePlayerEntry.loc[0, 'trainPlayNum'] < playsThreshold:\n",
    "                onePlayerEntry.loc[0, 'predict'] = onePlayerEntry.loc[0, 'trainAllPredictedMean']\n",
    "\n",
    "            playerTrain.append(onePlayerEntry)\n",
    "\n",
    "        ### testing data ###\n",
    "        players_2 = test_players[~test_players.isna()].unique()\n",
    "        players_2 = [player for player in players_2 if player in players]\n",
    "\n",
    "        for player in players_2:\n",
    "            id = np.array(test_players) == player\n",
    "            test_obs = test_label[id].mean()\n",
    "            \n",
    "            onePlayerEntry = pd.DataFrame([[week, player, np.sum(id), test_obs]], columns=['week', 'playerid', 'testPlayNum', 'testPlayerObservedMean'])\n",
    "            onePlayerEntry = pd.merge(onePlayerEntry, test_player_df, on=['week','playerid'], how='left')\n",
    "            onePlayerEntry = pd.merge(onePlayerEntry, team_df, on=['defenseTeam'], how='left')\n",
    "            playerTest.append(onePlayerEntry)\n",
    "            \n",
    "        #if week > 0:\n",
    "        #    break\n",
    "        break\n",
    "              \n",
    "    ### combine outputs\n",
    "    player_pred_df = pd.concat(playerTrain, ignore_index=True)\n",
    "\n",
    "    playerTest_df = pd.concat(playerTest, ignore_index=True)\n",
    "\n",
    "    player_df = pd.merge(player_pred_df, playerTest_df, on=['week', 'playerid'], how='right')\n",
    "\n",
    "    ### add defense team adjustment\n",
    "    player_df['predict_2'] = player_df.predict + (player_df.team_obs - player_df.team_pred)\n",
    "\n",
    "    print(player_pred_df.shape, playerTest_df.shape, player_df.shape)\n",
    "    \n",
    "    return(player_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QB_pass = playerRatesTesting(2015, 2018, 2019, features_transformed_2, label_pass, pbp_df, qb_info.playerid.tolist(), model_RF, \n",
    "                             0.9995, 0.99, 10, predictProb=True)\n",
    "QB_pass.to_csv('QB_pass_testing_data.csv', index=False)\n",
    "\n",
    "# MSE\n",
    "print('{:.5f} {:.5f}'.format(np.mean((QB_pass.trainPlayerSimpleMean - QB_pass.testPlayerObservedMean)**2), np.mean((QB_pass.predict_2 - QB_pass.testPlayerObservedMean)**2)) )\n",
    "\n",
    "# MAE\n",
    "print('{:.4f} {:.4f}'.format(np.mean(abs(QB_pass.trainPlayerSimpleMean - QB_pass.testPlayerObservedMean)), np.mean(abs(QB_pass.predict_2 - QB_pass.testPlayerObservedMean))) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QB_rush = playerRatesTesting(2015, 2018, 2019, features_transformed, label_rush, pbp_df, qb_info.playerid.tolist(), model_RF, \n",
    "                             0.9995, 0.99, 10, predictProb=True)\n",
    "QB_rush.to_csv('QB_rush_testing_data.csv', index=False)\n",
    "\n",
    "# MSE\n",
    "print('{:.5f} {:.5f}'.format(np.mean((QB_rush.trainPlayerSimpleMean - QB_rush.testPlayerObservedMean)**2), np.mean((QB_rush.predict_2 - QB_rush.testPlayerObservedMean)**2)) )\n",
    "\n",
    "# MAE\n",
    "print('{:.4f} {:.4f}'.format(np.mean(abs(QB_rush.trainPlayerSimpleMean - QB_rush.testPlayerObservedMean)), np.mean(abs(QB_rush.predict_2 - QB_rush.testPlayerObservedMean))) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QB_scramble = playerRatesTesting(2015, 2018, 2019, features_transformed, label_scramble, pbp_df, qb_info.playerid.tolist(), model_RF, \n",
    "                                 0.9995, 0.99, 10, predictProb=True)\n",
    "QB_scramble.to_csv('QB_scramble_testing_data.csv', index=False)\n",
    "\n",
    "# MSE\n",
    "print('{:.5f} {:.5f}'.format(np.mean((QB_scramble.trainPlayerSimpleMean - QB_scramble.testPlayerObservedMean)**2), np.mean((QB_scramble.predict_2 - QB_scramble.testPlayerObservedMean)**2)) )\n",
    "\n",
    "# MAE\n",
    "print('{:.4f} {:.4f}'.format(np.mean(abs(QB_scramble.trainPlayerSimpleMean - QB_scramble.testPlayerObservedMean)), np.mean(abs(QB_scramble.predict_2 - QB_scramble.testPlayerObservedMean))) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QB_sack = playerRatesTesting(2015, 2018, 2019, features_transformed, label_sack, pbp_df, qb_info.playerid.tolist(), model_RF, \n",
    "                             0.9995, 0.99, 10, predictProb=True)\n",
    "QB_sack.to_csv('QB_sack_testing_data.csv', index=False)\n",
    "\n",
    "# MSE\n",
    "print('{:.5f} {:.5f}'.format(np.mean((QB_sack.trainPlayerSimpleMean - QB_sack.testPlayerObservedMean)**2), np.mean((QB_sack.predict_2 - QB_sack.testPlayerObservedMean)**2)) )\n",
    "\n",
    "# MAE\n",
    "print('{:.4f} {:.4f}'.format(np.mean(abs(QB_sack.trainPlayerSimpleMean - QB_sack.testPlayerObservedMean)), np.mean(abs(QB_sack.predict_2 - QB_sack.testPlayerObservedMean))) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot between observation and prediction\n",
    "\n",
    "plt.figure(figsize=[9,9])\n",
    "plt.plot(QB_pass.testPlayerObservedMean, QB_pass.predict_2, 'o')\n",
    "plt.xlabel('QB pass %')\n",
    "plt.xlim(-0.05, 1.05)\n",
    "plt.ylabel('Prediction')\n",
    "plt.ylim(-0.05, 1.05)\n",
    "plt.plot( [-0.05,1.05],[-0.05,1.05] )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. MLFlow logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment('NFL-QB-rates-pbp')\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "with mlflow.start_run():\n",
    "\n",
    "    id_train = (pbp_df.season <= 2018).tolist()\n",
    "    id_test =  (pbp_df.season == 2019).tolist()\n",
    "    \n",
    "    X_train = features_transformed_2[id_train]\n",
    "    y_train = label_pass[id_train]\n",
    "    X_test = features_transformed_2[id_test]\n",
    "    y_test = label_pass[id_test]\n",
    "    \n",
    "    model = model_RF\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # get_params() returns a dictionary of {param_name: param_value} pairs.\n",
    "    model_params = model.get_params()\n",
    "\n",
    "    # Luckily, this is the exact format that mlflow's log_params() function takes!\n",
    "    mlflow.log_params(model_params)\n",
    "    \n",
    "    # Let's now evaluate our model's performance and log those metrics\n",
    "    metrics = {\n",
    "        'train_recall': recall_score(y_train, model.predict(X_train), average='macro'),\n",
    "        'test_recall':  recall_score(y_test,   model.predict(X_test),  average='macro'),\n",
    "        #'train_precision': precision_score(y_train, model.predict(X_train), average='macro'),\n",
    "        #'test_precision': precision_score(y_test, model.predict(X_test), average='macro'),\n",
    "        #'train_f1': f1_score(y_train, model.predict(X_train), average='macro'),\n",
    "        #'test_f1': f1_score(y_test, model.predict(X_test), average='macro'),\n",
    "    }\n",
    "    print(metrics)\n",
    "    mlflow.log_metrics(metrics)\n",
    "    \n",
    "    # log model: the mlflow.sklearn module (https://mlflow.org/docs/latest/python_api/mlflow.sklearn.html)\n",
    "    # provides lots of helpful tools for interacting with sklearn models. The function log_model() autoamtically\n",
    "    # creates an artifact from your trained sklearn model with all of the necessary info to reproduce your model.\n",
    "    mlflow.sklearn.log_model(model, '')\n",
    "    \n",
    "    mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(id_train)\n",
    "features_transformed_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Multi-nomial classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Label / Feature creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_pass = pbp_df.playtypeid.isin([1,2,9])\n",
    "\n",
    "id_rush = pbp_df.playtypeid.isin([4]) & (pbp_df.handofftype!='Q') & (pbp_df.rolloutlocation=='N')\n",
    "\n",
    "id_scramble = pbp_df.playtypeid.isin([4]) & (pbp_df.handofftype=='Q') & (pbp_df.rolloutlocation=='N')\n",
    "\n",
    "id_sack = pbp_df.playtypeid.isin([3])\n",
    "\n",
    "label = np.array([0] * len(id_pass))\n",
    "label[id_pass]     = 1\n",
    "label[id_rush]     = 2\n",
    "label[id_scramble] = 3\n",
    "label[id_sack]     = 4\n",
    "\n",
    "sum(label==0)\n",
    "\n",
    "id_train = (pbp_df.season <= 2019).tolist()\n",
    "id_test =  (pbp_df.season == 2019).tolist()\n",
    "\n",
    "X_train = features_transformed[id_train]\n",
    "y_train = label[id_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Model study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# multinomial classification model - logistic regression\n",
    "model = LogisticRegression(multi_class=\"multinomial\", solver='lbfgs', C=10, fit_intercept=True, max_iter=10000, tol=1e-4, random_state=42, class_weight=None)\n",
    "\n",
    "y_train_pred = cross_val_predict(model, X_train, y_train, cv=5, method='predict_proba')\n",
    "\n",
    "# binomial classification model\n",
    "model = LogisticRegression(solver='lbfgs', C=10, fit_intercept=True, max_iter=10000, tol=1e-4, random_state=42, class_weight=None)\n",
    "pass_train_pred = cross_val_predict(model, X_train, label_pass[id_train], cv=5, method='predict_proba')\n",
    "\n",
    "rush_train_pred = cross_val_predict(model, X_train, label_rush[id_train], cv=5, method='predict_proba')\n",
    "\n",
    "scramble_train_pred = cross_val_predict(model, X_train, label_scramble[id_train], cv=5, method='predict_proba')\n",
    "\n",
    "sack_train_pred = cross_val_predict(model, X_train, label_sack[id_train], cv=5, method='predict_proba')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given this sklearn is an old version, roc for multi-nomial classification is not ready yet!\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print(roc_auc_score(label_pass[id_train], pass_train_pred[:,1]))\n",
    "print(roc_auc_score(label_pass[id_train], y_train_pred[:,1]), '\\n')\n",
    "\n",
    "print(roc_auc_score(label_rush[id_train], rush_train_pred[:,1]))\n",
    "print(roc_auc_score(label_rush[id_train], y_train_pred[:,2]), '\\n')\n",
    "\n",
    "print(roc_auc_score(label_scramble[id_train], scramble_train_pred[:,1]))\n",
    "print(roc_auc_score(label_scramble[id_train], y_train_pred[:,3]), '\\n')\n",
    "\n",
    "print(roc_auc_score(label_sack[id_train], sack_train_pred[:,1]))\n",
    "print(roc_auc_score(label_sack[id_train], y_train_pred[:,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multinomial classification model - random forest\n",
    "model = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42, class_weight=None)\n",
    "\n",
    "y_train_pred = cross_val_predict(model, X_train, y_train, cv=5, method='predict_proba')\n",
    "\n",
    "# binomial classification model\n",
    "pass_train_pred = cross_val_predict(model, X_train, label_pass[id_train], cv=5, method='predict_proba')\n",
    "\n",
    "rush_train_pred = cross_val_predict(model, X_train, label_rush[id_train], cv=5, method='predict_proba')\n",
    "\n",
    "scramble_train_pred = cross_val_predict(model, X_train, label_scramble[id_train], cv=5, method='predict_proba')\n",
    "\n",
    "sack_train_pred = cross_val_predict(model, X_train, label_sack[id_train], cv=5, method='predict_proba')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(roc_auc_score(label_pass[id_train], pass_train_pred[:,1]))\n",
    "print(roc_auc_score(label_pass[id_train], y_train_pred[:,1]), '\\n')\n",
    "\n",
    "print(roc_auc_score(label_rush[id_train], rush_train_pred[:,1]))\n",
    "print(roc_auc_score(label_rush[id_train], y_train_pred[:,2]), '\\n')\n",
    "\n",
    "print(roc_auc_score(label_scramble[id_train], scramble_train_pred[:,1]))\n",
    "print(roc_auc_score(label_scramble[id_train], y_train_pred[:,3]), '\\n')\n",
    "\n",
    "print(roc_auc_score(label_sack[id_train], sack_train_pred[:,1]))\n",
    "print(roc_auc_score(label_sack[id_train], y_train_pred[:,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create output file\n",
    "outmat = pbp_df.loc[id_train, ['season','week','gamecode','eventtypeid','period','playid','secondsRemainingInPeriod','startpossessionteamid','down','yardsToGo','startYardsFromGoal','scoreDiff']]\n",
    "outmat['label']  = y_train\n",
    "outmat['b_pass'] = pass_train_pred[:,1]\n",
    "outmat['b_rush'] = rush_train_pred[:,1]\n",
    "outmat['b_scramble'] = scramble_train_pred[:,1]\n",
    "outmat['b_sack'] = sack_train_pred[:,1]\n",
    "m_mat = pd.DataFrame(y_train_pred[:, 1:], columns=['m_pass','m_rush','m_scramble','m_sack'])\n",
    "\n",
    "outmat = pd.concat([outmat, m_mat], axis=1)\n",
    "# outmat['isScramble'] = label_scramble[id]\n",
    "# outmat['pred_scramble_prob'] = re_prob_scramble[:,1]\n",
    "outmat.to_csv('QB_rate_comparison_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outmat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = (outmat.yardsToGo <= 1.1) & (outmat.down == 1) #& (~outmat.m_pass.isna())\n",
    "isPass = outmat.label == 1.0\n",
    "print(isPass[id].mean(), outmat[id].b_pass.mean(), outmat[id].m_pass.mean())\n",
    "outmat[id]\n",
    "\n",
    "isSack = outmat.label == 4.0\n",
    "print(isSack.mean(), outmat[isSack].b_sack.mean(), outmat[isSack].m_sack.mean())\n",
    "\n",
    "isScramble = outmat.label == 3.0\n",
    "print(isScramble.mean(), outmat[isScramble].b_sack.mean(), outmat[isScramble].m_sack.mean())\n",
    "\n",
    "isRush = outmat.label == 2.0\n",
    "print(isRush.mean(), outmat[isRush].b_rush.mean(), outmat[isRush].m_rush.mean())\n",
    "\n",
    "isPass = outmat.label == 1.0\n",
    "print(isPass.mean(), outmat[isPass].b_pass.mean(), outmat[isPass].m_pass.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(outmat[id].b_pass, outmat[id].m_pass)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nfl-rates-pbp",
   "language": "python",
   "name": "nfl-rates-pbp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
