{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only need to run once for each new instance\n",
    "!pip install pyathena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyathena import connect\n",
    "from pyathena.pandas_cursor import PandasCursor\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "import warnings\n",
    "\n",
    "#warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "pd.options.display.max_columns = 30\n",
    "pd.options.display.max_rows = 50\n",
    "\n",
    "# Define connection to DB\n",
    "conn = connect(\n",
    "    s3_staging_dir='s3://aws-athena-query-results-323906537337-us-east-1/',\n",
    "    region_name='us-east-1',\n",
    "    cursor_class=PandasCursor\n",
    "    )\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # thoughts\n",
    "\n",
    "# Only 2018~2019 seasons contain data for override values, thus for validation purpose.\n",
    "# could consider more seasons without validation data, but only for building model purpose (R2, MAE, cross-validation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Game data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # This implementation has issue due to data missing in tables\n",
    "\n",
    "# # Note: TE rarely gets rush carry\n",
    "\n",
    "# simple_query = f'''\n",
    "# with teamquery as\n",
    "# (\n",
    "# select season,\n",
    "#        eventmetadata.week week,\n",
    "#        eventmetadata.gameCode gamecode,  \n",
    "#        teamid, \n",
    "#        totalrushingattempts\n",
    "# from datalakefootball.team_aggregated_game_stats\n",
    "# where season>='2017' and eventmetadata.eventtypeid in (1,2)\n",
    "# order by season, week, gamecode, teamid\n",
    "# )\n",
    "\n",
    "# select\n",
    "#     cast(t1.season as integer) season,\n",
    "#     t1.eventmetadata.week week,\n",
    "#     t1.eventmetadata.gamecode gamecode,\n",
    "#     t1.teamid teamid,\n",
    "#     t1.player.playerid,\n",
    "#     t1.player.positionid,\n",
    "#     t1.rushertotalrushingattempts,\n",
    "#     t2.totalrushingattempts,\n",
    "    \n",
    "#     case when t2.totalrushingattempts = 0 \n",
    "#         then null\n",
    "#         else t1.rushertotalrushingattempts / cast (t2.totalrushingattempts as double)\n",
    "#         end as rushingShare,\n",
    "#     row_number() over (PARTITION BY\n",
    "#                          t1.season, t1.teamid, t1.eventmetadata.week, t1.player.positionid\n",
    "#                      ORDER BY\n",
    "#                         t1.rushertotalrushingattempts DESC) Rank\n",
    "# from\n",
    "#     datalakefootball.player_aggregated_game_stats as t1\n",
    "#     left join\n",
    "#     teamquery t2\n",
    "#     on t1.season = t2.season and \n",
    "#        t1.eventmetadata.gamecode = t2.gamecode and \n",
    "#        t1.teamid = t2.teamid\n",
    "# where t1.season >= '2017' and \n",
    "#      t1.eventmetadata.eventtypeid in (1,2) and\n",
    "#      t1.player.positionid in (1,8,9) -- 1:WR, 7:TE, 8:QB, 9:RB\n",
    "# order by season, week, gamecode, teamid\n",
    "# '''\n",
    "\n",
    "# if True:\n",
    "#     game_df2 = cursor.execute(simple_query).as_pandas()\n",
    "#     print(game_df2.info())\n",
    "# else:\n",
    "#     print(\"Failed to query!\")\n",
    "\n",
    "# print(game_df2.rushingShare.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a work-around\n",
    "# position_id may have some problem if a player changes position!\n",
    "\n",
    "simple_query = f'''\n",
    "with teamquery as\n",
    "(\n",
    "    select season,\n",
    "           eventmetadata.week week,\n",
    "           eventmetadata.gameCode gamecode,  \n",
    "           teamid, \n",
    "           totalrushingattempts\n",
    "    from datalakefootball.team_aggregated_game_stats\n",
    "    where season>='2017' and eventmetadata.eventtypeid in (1,2)\n",
    "    order by season, week, gamecode, teamid\n",
    "),\n",
    "playerquery as \n",
    "(\n",
    "  select playerid,\n",
    "           positions[1].positionid positionid\n",
    "  from datalakefootball.players\n",
    ")\n",
    "\n",
    "select\n",
    "    cast(t1.season as integer) season,\n",
    "    t1.eventmetadata.week week,\n",
    "    t1.eventmetadata.gamecode gamecode,\n",
    "    t1.eventmetadata.eventtypeid eventType,\n",
    "    t1.teamid teamid,\n",
    "    t1.playerid,\n",
    "    t3.positionid,\n",
    "    t1.playerstats.rushingstats.attempts rushertotalrushingattempts,\n",
    "    t2.totalrushingattempts,\n",
    "    \n",
    "    case when t2.totalrushingattempts = 0 \n",
    "        then null\n",
    "        else t1.playerstats.rushingstats.attempts / cast (t2.totalrushingattempts as double)\n",
    "        end as rushingShare,\n",
    "    row_number() over (PARTITION BY\n",
    "                         t1.season, t1.teamid, t1.eventmetadata.week, t3.positionid\n",
    "                     ORDER BY\n",
    "                        t1.playerstats.rushingstats.attempts DESC) Rank,\n",
    "    if (playerstats.inactives is not null, False, True) as isActive\n",
    "from\n",
    "    datalakefootball.player_stats_game as t1\n",
    "    left join teamquery t2\n",
    "    on t1.season = t2.season and \n",
    "       t1.eventmetadata.gamecode = t2.gamecode and \n",
    "       t1.teamid = t2.teamid\n",
    "    left join playerquery t3\n",
    "    on t1.playerid = t3.playerid \n",
    "where t1.season >= '2017' and \n",
    "     t1.eventmetadata.eventtypeid in (1,2) and\n",
    "     t3.positionid in (1,8,9) -- 1:WR, 7:TE, 8:QB, 9:RB\n",
    "order by season, week, gamecode, teamid\n",
    "'''\n",
    "\n",
    "if True:\n",
    "    game_df = cursor.execute(simple_query).as_pandas()\n",
    "    print(game_df.info())\n",
    "else:\n",
    "    print(\"Failed to query!\")\n",
    "\n",
    "print(game_df.rushingShare.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(game_df.totalrushingattempts > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check rush share distribution by player positions\n",
    "\n",
    "id = (game_df.rushertotalrushingattempts > 0)\n",
    "rushCounts = game_df[id].positionid.value_counts()\n",
    "print(rushCounts, '\\n\\n', rushCounts / rushCounts.sum(), sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalRushAttempts = game_df.rushertotalrushingattempts.sum()\n",
    "print(totalRushAttempts)\n",
    "\n",
    "id1 = game_df[id].positionid == 9\n",
    "RB_rushAttempts = sum(game_df[id].rushertotalrushingattempts[id1])\n",
    "print(\"RB rush share: {:}, {:.2%}\".format(RB_rushAttempts, RB_rushAttempts / totalRushAttempts) )\n",
    "\n",
    "id1 = game_df[id].positionid == 8\n",
    "QB_rushAttempts = sum(game_df[id].rushertotalrushingattempts[id1])\n",
    "print(\"QB rush share: {:},  {:.2%}\".format(QB_rushAttempts, QB_rushAttempts / totalRushAttempts) )\n",
    "\n",
    "id1 = game_df[id].positionid == 1\n",
    "WR_rushAttempts = sum(game_df[id].rushertotalrushingattempts[id1])\n",
    "print(\"WR rush share: {:},   {:.2%}\".format(WR_rushAttempts, WR_rushAttempts / totalRushAttempts) )\n",
    "\n",
    "\n",
    "# Pie chart, counter-clockwise:\n",
    "labels = ['RB', 'QB', 'WR']\n",
    "sizes = [RB_rushAttempts, QB_rushAttempts, WR_rushAttempts]\n",
    "explode = (0.1, 0, 0)  # only \"explode\" the 2nd slice (i.e. 'Hogs')\n",
    "\n",
    "plt.figure(figsize=[9,9])\n",
    "plt.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Expected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# we only have 2 years data 2018 ~ 2019 for research purpose\n",
    "\n",
    "simple_query = f'''\n",
    "select\n",
    "    eventmetadata.gamecode, -- may not be complete for some game\n",
    "    player.playerid,\n",
    "    rushingpercentage exp_rushingShare\n",
    "    -- overriderushingpercentage exp_rushingShare\n",
    "from datalakefootball.player_expected_rates\n",
    "where \n",
    "    season >= '2018' and \n",
    "    eventmetadata.eventtypeid in (1,2) and\n",
    "    player.positionid in (1,8,9) and version='override'\n",
    "order by season\n",
    "'''\n",
    "\n",
    "if True:\n",
    "    exp_df = cursor.execute(simple_query).as_pandas()\n",
    "    print(exp_df.info())\n",
    "else:\n",
    "    print(\"Failed to query!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "game_df = pd.merge(game_df, exp_df, on=['gamecode','playerid'], how='left')\n",
    "game_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.3 Ytd data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # # This implementation has issue due to data missing in tables\n",
    "\n",
    "# # In order to account for injury situation, team_aggregated_game_stats is used instead of team_aggregated_ytd_stats\n",
    "\n",
    "# simple_query = f'''\n",
    "# with teamquery as\n",
    "# (\n",
    "# select eventmetadata.gameCode gamecode,  \n",
    "#        teamid, \n",
    "#        totalrushingattempts game_totalRushingAttempts\n",
    "# from datalakefootball.team_aggregated_game_stats\n",
    "# where season>='2017' and eventmetadata.eventtypeid in (1,2)\n",
    "# )\n",
    "\n",
    "# select \n",
    "#   cast(t1.season as integer) season, \n",
    "#   t1.eventmetadata.week, \n",
    "#   t1.eventmetadata.gamecode, \n",
    "#   t1.teamid,\n",
    "#   t1.player.playerid,\n",
    "#   t1.player.positionid,\n",
    "#   t1.rushertotalrushingattempts ytd_rushertotalrushingattempts,\n",
    "#   t2.game_totalRushingAttempts,\n",
    "#   row_number() over (PARTITION BY\n",
    "#                          t1.season, t1.teamid, t1.eventmetadata.week, t1.player.positionid\n",
    "#                      ORDER BY\n",
    "#                          t1.rushertotalrushingattempts DESC) ytd_rank\n",
    "# from datalakefootball.player_aggregated_ytd_stats t1\n",
    "#     left join teamquery t2\n",
    "#     on t1.eventmetadata.gamecode = t2.gamecode and \n",
    "#        t1.teamid=t2.teamid\n",
    "# where \n",
    "#     t1.season >='2017' and \n",
    "#     t1.player.positionid in (1,8,9) and \n",
    "#     t1.eventmetadata.eventtypeid in (1,2) and\n",
    "#     t1.eventmetadata.week is not null -- when a week is missing, the player may not be active\n",
    "# order by season, week, gamecode, teamid\n",
    "# '''\n",
    "\n",
    "# if True:\n",
    "#     ytd_df = cursor.execute(simple_query).as_pandas()\n",
    "#     print(ytd_df.info())\n",
    "# else:\n",
    "#     print(\"Failed to query!\")\n",
    "    \n",
    "\n",
    "# # prepare ytd_rushingattempts for each player. in this way, no update for missed games\n",
    "# gd = ytd_df.groupby(['season','playerid'])\n",
    "\n",
    "# ytd_df['ytd_totalRushingAttempts'] = gd.game_totalrushingattempts.cumsum()\n",
    "# ytd_df['ytd_totalRushingAttempts'] = gd.ytd_totalRushingAttempts.shift(1)\n",
    "\n",
    "# ytd_df['ytd_rushingShare'] = ytd_df.ytd_rushertotalrushingattempts / ytd_df.ytd_totalRushingAttempts\n",
    "\n",
    "\n",
    "# # merge ytd data into game data\n",
    "# game_df = pd.merge(game_df, ytd_df[['gamecode','playerid',\n",
    "#                                     'ytd_rank',\n",
    "#                                    'ytd_rushertotalrushingattempts',\n",
    "#                                    'ytd_totalRushingAttempts',\n",
    "#                                    'ytd_rushingShare']], \n",
    "#                     on=['gamecode','playerid'], how='left')\n",
    "# game_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a work-around\n",
    "\n",
    "simple_query = f'''\n",
    "select eventmetadata.gameCode gamecode,  \n",
    "       teamid, \n",
    "       totalrushingattempts game_totalRushingAttempts\n",
    "from datalakefootball.team_aggregated_game_stats\n",
    "where season>='2017' and eventmetadata.eventtypeid in (1,2)\n",
    "'''\n",
    "\n",
    "if True:\n",
    "    ytd_team_df = cursor.execute(simple_query).as_pandas()\n",
    "    print(ytd_team_df.info())\n",
    "else:\n",
    "    print(\"Failed to query!\")\n",
    "\n",
    "game_df = pd.merge(game_df, ytd_team_df, on=['gamecode','teamid'], how='left')\n",
    "game_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we should not simply fill the NA value since it may indicate missing the game\n",
    "id = game_df.rushertotalrushingattempts.isna()\n",
    "game_df['game_totalRushingAttempts'][id] = np.nan\n",
    "\n",
    "gd = game_df.groupby(['season', 'playerid'])\n",
    "\n",
    "game_df['ytd_totalRushingAttempts'] = gd.game_totalRushingAttempts.cumsum()\n",
    "game_df['ytd_rushertotalrushingattempts'] = gd.rushertotalrushingattempts.cumsum()\n",
    "\n",
    "game_df['ytd_rushingShare'] = game_df.ytd_rushertotalrushingattempts / game_df.ytd_totalRushingAttempts\n",
    "\n",
    "# For missing games, fill players' rushing share with previous game results\n",
    "game_df[['ytd_totalRushingAttempts','ytd_rushertotalrushingattempts','ytd_rushingShare']] =\\\n",
    "        gd[['ytd_totalRushingAttempts','ytd_rushertotalrushingattempts','ytd_rushingShare']].fillna(method='ffill')\n",
    "\n",
    "game_df[['ytd_totalRushingAttempts', 'ytd_rushertotalrushingattempts', 'ytd_rushingShare']] = \\\n",
    "        gd[['ytd_totalRushingAttempts', 'ytd_rushertotalrushingattempts', 'ytd_rushingShare']].shift(1)\n",
    "\n",
    "game_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_df[['game_totalRushingAttempts','rushertotalrushingattempts','rushingShare','ytd_rushingShare','ytd_totalRushingAttempts','ytd_rushertotalrushingattempts']] =\\\n",
    "        game_df[['game_totalRushingAttempts','rushertotalrushingattempts','rushingShare','ytd_rushingShare','ytd_totalRushingAttempts','ytd_rushertotalrushingattempts']].fillna(0)\n",
    "\n",
    "game_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.4 Previous game data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# previous (active) game rush share, regardless of team id\n",
    "\n",
    "gd = game_df.groupby(['season', 'playerid'])\n",
    "game_df['prev_rushingShare'] = gd.rushingShare.shift(1)\n",
    "game_df.info()\n",
    "game_df.prev_rushingShare = game_df.prev_rushingShare.fillna(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.5 Prepare ytd data by position rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create ytd targetShare by position rank\n",
    "# only ytd data is used, no baseline information is used!\n",
    "# 'gamecode' and 'playerid' are irrelevant here!\n",
    "\n",
    "ytd_byPosRank_df = game_df[['season','week','gamecode','teamid','playerid','positionid',\n",
    "                   'rushertotalrushingattempts','totalrushingattempts','Rank']].copy()\n",
    "\n",
    "gd = ytd_byPosRank_df.groupby(['season','teamid','positionid','Rank'])\n",
    "\n",
    "ytd_byPosRank_df['ytd_rushingShareByPositionRank'] = gd.rushertotalrushingattempts.cumsum() / \\\n",
    "                                            gd.totalrushingattempts.cumsum()\n",
    "ytd_byPosRank_df['ytd_rushingShareByPositionRank'] = gd.ytd_rushingShareByPositionRank.shift(1)\n",
    "\n",
    "ytd_byPosRank_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.6 Identify weekly injury"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify weekly injury. If a top player is out, the remaining players on that position will use ytd_targetShare according to adjusted position rank\n",
    "\n",
    "# to be addressed (TBA) cases:\n",
    "# (1) Player 835814 with zero baseline info, who joined team 334 in mid of 2019 and became lead RB.\n",
    "#     the current algo has difficulty to catch up and update this player to be new #1 RB\n",
    "#     later, 880548 returns who is a top RB of the team and out temporarily due to injury -> percentage normalization issue!!!\n",
    "# (2) Player normalization\n",
    "#     all active player's share should be scaled accoring to total rushing shares by individual RB and total rushing share by RB position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to calculate target share by ranks for each positin type when there is a roster change\n",
    "# return a list of objects, each object contains targetShares for players for a game when roster change happens\n",
    "# Note: there is a bias to use targetShare by position to estimate each player's performance!!!\n",
    "\n",
    "# For rush share, we may have to rely on previous game ranking instead of ytd_ranking!\n",
    "\n",
    "def calculateRushingShareAdjByRank(game_df, positionIds, seasons, printDetails=False):\n",
    "\n",
    "    adjustedRates = []\n",
    "    \n",
    "    teams = game_df.teamid.unique()\n",
    "\n",
    "    for season in seasons:\n",
    "        for team in teams:\n",
    "            for positionId in positionIds:\n",
    "                print(team)\n",
    "\n",
    "                id = (game_df.season==season) & (game_df.teamid==team) & (game_df.positionid==positionId)\n",
    "                one_team = game_df[id].copy()\n",
    "\n",
    "                # ranking_data is used to dynamically track ytd players' ranking for certain position\n",
    "                ranking_data = []\n",
    "\n",
    "                for i,week in enumerate(one_team.week.unique()):\n",
    "                    id = (one_team.week == week) & (one_team.isActive)\n",
    "\n",
    "                    if i == 0:\n",
    "                        ranking_data = one_team.loc[id, ['rushertotalrushingattempts','playerid']].copy()\n",
    "                        ranking_data['ytd_rank'] = -1\n",
    "                        ranking_data.set_index('playerid', inplace=True)\n",
    "                        continue\n",
    "\n",
    "                    data = one_team.loc[id, ['teamid','gamecode','positionid','playerid',\n",
    "                                        'rushertotalrushingattempts','rushingShare','ytd_rushingShare']].copy()\n",
    "                    data.set_index('playerid', inplace=True)\n",
    "\n",
    "                    # add according to playerid index\n",
    "                    ranking_data = ranking_data.add(data[['rushertotalrushingattempts']], fill_value=0)\n",
    "                    ranking_data.sort_values('rushertotalrushingattempts', inplace=True, ascending=False)\n",
    "                    ranking_data.loc[:,'ytd_rank'] = np.arange(len(ranking_data)) + 1\n",
    "                    \n",
    "                    #print(week, ranking_data)\n",
    "\n",
    "                    current_week_data = one_team[id]\n",
    "                    activeMajorPlayers = current_week_data.playerid[current_week_data.rushertotalrushingattempts > 0]\n",
    "\n",
    "                    #check if any top (1) player(s) is missing for this week\n",
    "                    missingPlayers = [player for player in ranking_data.index.values\n",
    "                                     #if player not in current_week_data.playerid.values and\n",
    "                                      if player not in activeMajorPlayers.values and\n",
    "                                        ranking_data.loc[player].ytd_rank <= 2]\n",
    "\n",
    "                    #print(current_week_data[['week','playerid','rushertotalrushingattempts','isActive']])\n",
    "                    \n",
    "                    if missingPlayers:\n",
    "                        if printDetails:\n",
    "                            for player in missingPlayers:\n",
    "                                print(week, player, ranking_data.loc[player].ytd_rank)\n",
    "                                print(ranking_data)\n",
    "\n",
    "                        # re-arrange ranks of active players to reflect currently predicted rank\n",
    "                        data['onFieldRank'] = ranking_data.loc[data.index].ytd_rank.\\\n",
    "                                                rank(method='first', na_option='bottom')\n",
    "                        data = data.astype({'onFieldRank':'int64'})\n",
    "                        data.reset_index(inplace=True)\n",
    "\n",
    "                        # merge target%_by_rank into data\n",
    "                        data = pd.merge(data, \n",
    "                                        ytd_byPosRank_df[['teamid','gamecode','positionid','Rank',\n",
    "                                                 'ytd_rushingShareByPositionRank']], \n",
    "                                        left_on=['teamid','gamecode','positionid','onFieldRank'],\n",
    "                                        right_on=['teamid','gamecode','positionid','Rank'], how='left')\n",
    "                        \n",
    "                        # adjustment\n",
    "                        #data.ytd_targetShareByPositionRank = data.ytd_targetShareByPositionRank * 0.9\n",
    "\n",
    "                        adjustedRates.append(data)\n",
    "\n",
    "    return(adjustedRates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we only consider the position rank adjustment for running back position\n",
    "\n",
    "re = calculateRushingShareAdjByRank(game_df, positionIds=[9], seasons=[2018,2019], printDetails=True)\n",
    "\n",
    "adjustedRates = pd.concat(re, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_df = pd.merge(game_df, \n",
    "                 adjustedRates[['teamid','gamecode','playerid','onFieldRank','ytd_rushingShareByPositionRank']], \n",
    "                 on=['teamid','gamecode','playerid'], \n",
    "                 how='left')\n",
    "game_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create a new column 'ytd_targetShareAdj' to contain ytd data with adjustment by injury situation\n",
    "game_df['ytd_rushingShareAdj'] = game_df.ytd_rushingShare\n",
    "\n",
    "id = game_df.ytd_rushingShareByPositionRank.isnull()\n",
    "game_df.ytd_rushingShareAdj[~id] = game_df.ytd_rushingShareByPositionRank[~id] \n",
    "\n",
    "game_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new method has some improvement from ytd baseline for running back\n",
    "\n",
    "id = (game_df.season.isin([2018,2019])) & (game_df.positionid.isin([9])) & \\\n",
    "         (game_df.isActive) & (~game_df.exp_rushingShare.isnull()) & \\\n",
    "         (~ game_df.ytd_rushingShareByPositionRank.isnull()) \n",
    "\n",
    "print('{:.3f}'.format(abs(game_df[id].rushingShare - game_df[id].exp_rushingShare).mean()) )\n",
    "\n",
    "print('{:.3f}'.format(abs(game_df[id].rushingShare - game_df[id].ytd_rushingShare).mean()) )\n",
    "\n",
    "print('{:.3f}\\n'.format(abs(game_df[id].rushingShare - game_df[id].ytd_rushingShareByPositionRank).mean()) )\n",
    "\n",
    "\n",
    "id = (game_df.season.isin([2018,2019])) & (game_df.positionid.isin([9])) &\\\n",
    "        (game_df.isActive) & (~game_df.exp_rushingShare.isnull())\n",
    "\n",
    "print('{:.3f}'.format(abs(game_df.rushingShare - game_df.exp_rushingShare)[id].mean()) )\n",
    "\n",
    "print('{:.3f}'.format(abs(game_df.rushingShare - game_df.ytd_rushingShare)[id].mean()) )\n",
    "\n",
    "print('{:.3f}'.format(abs(game_df.rushingShare - game_df.ytd_rushingShareAdj)[id].mean()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.7 Prepare baseline data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create baseline case from ytd data\n",
    "# Note: each year, many new players join the league without baseline-info;\n",
    "#       many players retire so their baseline won't be in use for next year\n",
    "\n",
    "baseline_df = game_df[['season','playerid',\n",
    "                       'ytd_totalRushingAttempts',\n",
    "                       'ytd_rushertotalrushingattempts',\n",
    "                       'ytd_rushingShare',\n",
    "                       'ytd_rushingShareAdj']].copy()\n",
    "baseline_df = baseline_df.groupby(['season','playerid']).tail(1)\n",
    "\n",
    "baseline_df.rename(columns={\n",
    "                            'ytd_totalRushingAttempts':'base_totalRushingAttempts',\n",
    "                            'ytd_rushertotalrushingattempts':'base_rushertotalrushingattempts',\n",
    "                            'ytd_rushingShare':'base_rushingShare',\n",
    "                            'ytd_rushingShareAdj':'base_rushingShareAdj'},\n",
    "                                inplace=True)\n",
    "\n",
    "baseline_df.season = baseline_df.season + 1\n",
    "\n",
    "# merge baseline info into game_df, in this case, we will lose 2017\n",
    "\n",
    "game_df = pd.merge(game_df, baseline_df, on=['season','playerid'], how='left')\n",
    "game_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = game_df.season.isin([2018, 2019])\n",
    "game_df = game_df[id]\n",
    "\n",
    "game_df[['base_rushingShare','base_rushingShareAdj','base_totalRushingAttempts','base_rushertotalrushingattempts']]=\\\n",
    "    game_df[['base_rushingShare','base_rushingShareAdj','base_totalRushingAttempts','base_rushertotalrushingattempts']].fillna(0)\n",
    "\n",
    "game_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.8 weighted ytd data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted historical values\n",
    "# note the fill of na is after the target percentage has been calculated\n",
    "\n",
    "alpha = 5.0\n",
    "\n",
    "w = game_df.ytd_totalRushingAttempts * alpha / \\\n",
    "        (game_df.ytd_totalRushingAttempts * alpha + game_df.base_totalRushingAttempts)\n",
    "\n",
    "id = (game_df.ytd_totalRushingAttempts==0) & (game_df.base_totalRushingAttempts==0)\n",
    "w[id] = 1.0\n",
    "\n",
    "game_df['w_rushingShareAdj'] = game_df.ytd_rushingShareAdj * w + game_df.base_rushingShareAdj * (1-w)\n",
    "\n",
    "game_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.9 Share Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create rush share by position id\n",
    "# seems there is some issue with following algo\n",
    "# It should be: the average of team's rush share for RB / the sum of ytd_rushingShareAdj for active RB\n",
    "\n",
    "gd = game_df.groupby(['season','teamid','positionid','week'] )\n",
    "tmp = gd.rushertotalrushingattempts.sum() / gd.totalrushingattempts.median()\n",
    "tmp.rename('teamPositionRushShare', inplace = True)\n",
    "tmp = tmp.to_frame() #.reset_index()\n",
    "\n",
    "gd = tmp.groupby(level=['season','teamid','positionid'], as_index=False, group_keys=False)\n",
    "tmp2 = gd.expanding().mean()\n",
    "tmp2.rename(columns = {'teamPositionRushShare':'m_teamPositionRushShare'}, inplace = True)\n",
    "\n",
    "gd = tmp2.groupby(level = ['season','teamid','positionid'], as_index=False, group_keys=False)\n",
    "tmp2 = gd.shift(1)\n",
    "tmp['m_teamPositionRushShare'] = tmp2['m_teamPositionRushShare']\n",
    "\n",
    "tmp.reset_index()\n",
    "\n",
    "game_df = pd.merge(game_df, tmp, on=['season','teamid','week','positionid'], how='left')\n",
    "\n",
    "game_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized w_rushShareAdj, only for RB\n",
    "game_df['w_rushingShareAdj_norm'] = game_df.w_rushingShareAdj\n",
    "\n",
    "id = game_df.positionid==9\n",
    "game_df.loc[id,'w_rushingShareAdj_norm'] = game_df[id].teamPositionRushShare / game_df[id].m_teamPositionRushShare * game_df[id].w_rushingShareAdj\n",
    "\n",
    "game_df[game_df.positionid==9].w_rushingShareAdj_norm.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Feature preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy in case we need to re-run the following steps\n",
    "#tmp = game_df.copy()\n",
    "game_df = tmp.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep cases that players are active and have override predictions\n",
    "# the filter of skipping starting weeks and zero output game is very important!!!\n",
    "\n",
    "id = (~ game_df.exp_rushingShare.isnull()) & (game_df.isActive) & (game_df.week > 1) & (game_df.rushertotalrushingattempts > 0)  #& (game_df.positionid==9)\n",
    "game_df = game_df[id]\n",
    "game_df.week = game_df.week.astype('float64')\n",
    "game_df.ytd_rushertotalrushingattempts = game_df.ytd_rushertotalrushingattempts.astype('float64')\n",
    "\n",
    "#game_df.info()\n",
    "game_df.rushingShare.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Feature transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_fields = [\n",
    "                'w_rushingShareAdj_norm',\n",
    "                'prev_rushingShare',\n",
    "                'ytd_rushertotalrushingattempts',\n",
    "                'week'\n",
    "             ]\n",
    "\n",
    "cat_fields = [\n",
    "                'eventType',\n",
    "                'positionid',\n",
    "             ]\n",
    "              \n",
    "label = game_df.rushingShare\n",
    "\n",
    "# StandardScaler version\n",
    "transform_pipeline = ColumnTransformer(transformers=[\n",
    "                                            ('num', StandardScaler(), num_fields),\n",
    "                                            ('cat', OneHotEncoder(categories='auto'), cat_fields)\n",
    "                                        ])\n",
    "features_transformed = transform_pipeline.fit_transform(game_df)\n",
    "\n",
    "\n",
    "# None-StandardScaler version\n",
    "transform_pipeline_2 = ColumnTransformer(transformers=[\n",
    "                                            ('num', 'passthrough', num_fields),\n",
    "                                            ('cat', OneHotEncoder(categories='auto'), cat_fields)\n",
    "                                        ])\n",
    "features_transformed_2 = transform_pipeline_2.fit_transform(game_df)\n",
    "\n",
    "feature_names = num_fields\n",
    "feature_names.extend(transform_pipeline_2.named_transformers_.cat.get_feature_names())\n",
    "\n",
    "if type(features_transformed_2) == np.ndarray:\n",
    "    features_transformed_2 = pd.DataFrame(features_transformed_2, columns=feature_names)\n",
    "else:\n",
    "    features_transformed_2 = pd.DataFrame(features_transformed_2.toarray(), columns=feature_names)\n",
    "\n",
    "cat_one_hot_fields = list(transform_pipeline.named_transformers_.cat.get_feature_names())\n",
    "print(cat_one_hot_fields)\n",
    "pd.DataFrame(features_transformed).info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Benchmark model study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Rush share summary:\")\n",
    "printBenchmarkModelPerformance(game_df, [1,8,9])\n",
    "\n",
    "print(\"\\nRush share summary for WRs:\")\n",
    "printBenchmarkModelPerformance(game_df, [1])\n",
    "\n",
    "print(\"\\nRush share summary for QBs:\")\n",
    "printBenchmarkModelPerformance(game_df, [8])\n",
    "\n",
    "print(\"\\nRush share summary for RBs:\")\n",
    "printBenchmarkModelPerformance(game_df, [9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printBenchmarkModelPerformance(data, positionId):\n",
    "    id = data.positionid.isin(positionId)\n",
    "    \n",
    "    data_df = data[id].copy()\n",
    "    \n",
    "    print(data_df.rushingShare.describe(), '\\n')\n",
    "    \n",
    "    re = (data_df.rushingShare - data_df.exp_rushingShare)\n",
    "    r2 = 1 - sum(re**2)/sum((data_df.rushingShare - np.mean(data_df.rushingShare))**2)\n",
    "    print(\"                    {}     {}\".format('MAE', 'R2') )\n",
    "    print(\"Override model:     {:.4f}  {:.1%}\".format(abs(re).mean(), r2) )\n",
    "\n",
    "    re = (data_df.rushingShare - data_df.ytd_rushingShare)\n",
    "    r2 = 1 - sum(re**2)/sum((data_df.rushingShare - np.mean(data_df.rushingShare))**2)\n",
    "    print(\"ytd model:          {:.4f}  {:.1%}\".format(abs(re).mean(), r2) )\n",
    "\n",
    "    re = (data_df.rushingShare - data_df.w_rushingShareAdj)\n",
    "    r2 = 1 - sum(re**2)/sum((data_df.rushingShare - np.mean(data_df.rushingShare))**2)\n",
    "    print(\"Weighted ytd model: {:.4f}, {:.1%}\".format(abs(re).mean(), r2) )\n",
    "    \n",
    "    re = (data_df.rushingShare - data_df.w_rushingShareAdj_norm)\n",
    "    r2 = 1 - sum(re**2)/sum((data_df.rushingShare - np.mean(data_df.rushingShare))**2)\n",
    "    print(\"W/N ytd model:      {:.4f}, {:.1%}\".format(abs(re).mean(), r2) )\n",
    "\n",
    "    return()\n",
    "\n",
    "print(\"Rush share summary:\")\n",
    "printBenchmarkModelPerformance(game_df, [1,8,9])\n",
    "\n",
    "print(\"\\nRush share summary for WRs:\")\n",
    "printBenchmarkModelPerformance(game_df, [1])\n",
    "\n",
    "print(\"\\nRush share summary for QBs:\")\n",
    "printBenchmarkModelPerformance(game_df, [8])\n",
    "\n",
    "print(\"\\nRush share summary for RBs:\")\n",
    "printBenchmarkModelPerformance(game_df, [9])\n",
    "\n",
    "#game_df.rushingShare.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Machine learning model study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate models\n",
    "MAE = make_scorer(mean_absolute_error)\n",
    "folds = 5\n",
    "\n",
    "model_linear = SGDRegressor(max_iter=10000, tol=1e-4)\n",
    "\n",
    "model_svr = LinearSVR(random_state=0, tol=1e-4, max_iter=100000)\n",
    "\n",
    "model_rf = RandomForestRegressor(n_estimators=200, max_depth=30, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('                   MAE     R2' )\n",
    "\n",
    "MAE_linear = cross_val_score(model_linear,\n",
    "    features_transformed,\n",
    "    label,\n",
    "    cv=folds,\n",
    "    scoring=(MAE))\n",
    "R2_linear = cross_val_score(model_linear,\n",
    "    features_transformed,\n",
    "    label,\n",
    "    cv=folds,\n",
    "    scoring=('r2'))\n",
    "print('Linear regression: {:.4f}  {:.1%}'.format(np.mean(MAE_linear), np.mean(R2_linear)))\n",
    "\n",
    "\n",
    "MAE_rf = cross_val_score(model_rf,\n",
    "    features_transformed_2,\n",
    "    label,\n",
    "    cv=folds,\n",
    "    scoring=MAE)\n",
    "R2_rf = cross_val_score(model_rf,\n",
    "    features_transformed_2,\n",
    "    label,\n",
    "    cv=folds,\n",
    "    scoring=('r2'))\n",
    "print('RF regression:     {:.4f}  {:.1%}'.format(np.mean(MAE_rf), np.mean(R2_rf)))\n",
    "\n",
    "MAE_svr = cross_val_score(model_svr,\n",
    "    features_transformed,\n",
    "    label,\n",
    "    cv=folds,\n",
    "    scoring=MAE)\n",
    "R2_svr = cross_val_score(model_svr,\n",
    "    features_transformed,\n",
    "    label,\n",
    "    cv=folds,\n",
    "    scoring=('r2'))\n",
    "print('SV regression:     {:.4f}  {:.1%}'.format(np.mean(MAE_svr), np.mean(R2_svr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importance study\n",
    "\n",
    "regr = RandomForestRegressor(n_estimators=200, max_depth=20, random_state=0)\n",
    "regr.fit(features_transformed, label)\n",
    "\n",
    "cat_one_hot_fields = list(transform_pipeline.named_transformers_.cat.get_feature_names())\n",
    "feature_score = pd.DataFrame([num_fields + cat_one_hot_fields,regr.feature_importances_], \n",
    "                             index=['feature','importance']).transpose()\n",
    "feature_score.sort_values(by='importance',ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regression model interpretation\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "mod = sm.OLS(np.array(label), features_transformed_2, missing='drop')\n",
    "res = mod.fit()\n",
    "\n",
    "mae = np.abs(res.resid).mean()\n",
    "\n",
    "print('{:.3f}'.format(mae) )\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.resid.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[9,9])\n",
    "plt.plot(game_df.rushingShare, res.fittedvalues, 'o')\n",
    "plt.xlabel('rushing share')\n",
    "plt.xlim(0.0, 1.05)\n",
    "plt.ylabel('prediction')\n",
    "plt.ylim(0.0, 1.05)\n",
    "plt.plot( [0,1],[0,1] )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_df['fitted_rushingShare']=res.fittedvalues.values\n",
    "game_df.to_csv(\"rush_share_modeling_results.csv\")\n",
    "game_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Large error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = (game_df.rushingShare - game_df.w_rushingShareAdj > 0.4) & (game_df.season == 2019)\n",
    "sum(id)\n",
    "game_df[id]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nfl-usage-models",
   "language": "python",
   "name": "nfl-usage-models"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
